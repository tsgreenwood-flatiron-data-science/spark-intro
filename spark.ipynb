{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: Getting Started\n",
    " * These instructions require a Mac with [Anaconda3](https://anaconda.com/) and [Homebrew](https://brew.sh/) installed.\n",
    " * Useful for small data only. For larger data, try [Databricks](https://databricks.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisites & Installation\n",
    "\n",
    "Run these commands in your terminal (just once).\n",
    "\n",
    "```bash\n",
    "# Make Homebrew aware of old versions of casks\n",
    "brew tap caskroom/versions\n",
    "\n",
    "# Install Java 1.8 (OpenJDK 8)\n",
    "brew cask install adoptopenjdk8\n",
    "\n",
    "# Install the current version of Spark\n",
    "brew install apache-spark\n",
    "\n",
    "# Install wget\n",
    "brew install wget\n",
    "\n",
    "# Install Py4J (connects PySpark to the Java Virtual Machine)\n",
    "pip install py4j\n",
    "\n",
    "# Add JAVA_HOME to .bash_profile (makes Java 1.8 your default JVM)\n",
    "echo \"export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\" >> ~/.bash_profile\n",
    "\n",
    "# Add SPARK_HOME to .bash_profile\n",
    "export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.3/libexec\n",
    "echo \"export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.3/libexec\" >> ~/.bash_profile\n",
    "\n",
    "# Add PySpark to PYTHONPATH\n",
    "echp \"export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\" >> ~/.ba\n",
    "\n",
    "# Update current environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a SparkSession with a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.137.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10dd77550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.137.1.182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download some Amazon reviews (Toys & Games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE\n",
    "Download data (run this only once)\n",
    "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz\n",
    "!gunzip reviews_Toys_and_Games_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice this is a local file. There are computer clusters don't have local files so be careful.\n",
    "jf = spark.read.json('data/reviews_Toys_and_Games_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at schema\n",
    "jf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+--------------+--------------------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|  reviewerName|             summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------+--------------------+--------------+\n",
      "|0439893577| [0, 0]|    5.0|I like the item p...|01 29, 2014|A1VXOAVRGKGEAK|         Angie|      Magnetic board|    1390953600|\n",
      "|0439893577| [1, 1]|    4.0|Love the magnet e...|03 28, 2014| A8R62G708TSCM|       Candace|it works pretty g...|    1395964800|\n",
      "|0439893577| [1, 1]|    5.0|Both sides are ma...|01 28, 2013|A21KH420DK0ICA|capemaychristy|          love this!|    1359331200|\n",
      "|0439893577| [0, 0]|    5.0|Bought one a few ...| 02 8, 2014| AR29QK6HPFYZ4|          dcrm|   Daughters love it|    1391817600|\n",
      "|0439893577| [1, 1]|    4.0|I have a stainles...| 05 5, 2014| ACCH8EOML6FN5|          DoyZ|Great to have so ...|    1399248000|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jf.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>01 29, 2014</td>\n",
       "      <td>A1VXOAVRGKGEAK</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Magnetic board</td>\n",
       "      <td>1390953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>03 28, 2014</td>\n",
       "      <td>A8R62G708TSCM</td>\n",
       "      <td>Candace</td>\n",
       "      <td>it works pretty good for moving to different a...</td>\n",
       "      <td>1395964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Both sides are magnetic.  A real plus when you...</td>\n",
       "      <td>01 28, 2013</td>\n",
       "      <td>A21KH420DK0ICA</td>\n",
       "      <td>capemaychristy</td>\n",
       "      <td>love this!</td>\n",
       "      <td>1359331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought one a few years ago for my daughter and...</td>\n",
       "      <td>02 8, 2014</td>\n",
       "      <td>AR29QK6HPFYZ4</td>\n",
       "      <td>dcrm</td>\n",
       "      <td>Daughters love it</td>\n",
       "      <td>1391817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I have a stainless steel refrigerator therefor...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>ACCH8EOML6FN5</td>\n",
       "      <td>DoyZ</td>\n",
       "      <td>Great to have so he can play with his alphabet...</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  0439893577  [0, 0]      5.0   \n",
       "1  0439893577  [1, 1]      4.0   \n",
       "2  0439893577  [1, 1]      5.0   \n",
       "3  0439893577  [0, 0]      5.0   \n",
       "4  0439893577  [1, 1]      4.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I like the item pricing. My granddaughter want...  01 29, 2014   \n",
       "1  Love the magnet easel... great for moving to d...  03 28, 2014   \n",
       "2  Both sides are magnetic.  A real plus when you...  01 28, 2013   \n",
       "3  Bought one a few years ago for my daughter and...   02 8, 2014   \n",
       "4  I have a stainless steel refrigerator therefor...   05 5, 2014   \n",
       "\n",
       "       reviewerID    reviewerName  \\\n",
       "0  A1VXOAVRGKGEAK           Angie   \n",
       "1   A8R62G708TSCM         Candace   \n",
       "2  A21KH420DK0ICA  capemaychristy   \n",
       "3   AR29QK6HPFYZ4            dcrm   \n",
       "4   ACCH8EOML6FN5            DoyZ   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                     Magnetic board      1390953600  \n",
       "1  it works pretty good for moving to different a...      1395964800  \n",
       "2                                         love this!      1359331200  \n",
       "3                                  Daughters love it      1391817600  \n",
       "4  Great to have so he can play with his alphabet...      1399248000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instead of loading this file in over again every time we want to look at it, we'll use\n",
    "jf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='0439893577', helpful=[0, 0], overall=5.0, reviewText='I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.', reviewTime='01 29, 2014', reviewerID='A1VXOAVRGKGEAK', reviewerName='Angie', summary='Magnetic board', unixReviewTime=1390953600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = jf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>01 29, 2014</td>\n",
       "      <td>A1VXOAVRGKGEAK</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Magnetic board</td>\n",
       "      <td>1390953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>03 28, 2014</td>\n",
       "      <td>A8R62G708TSCM</td>\n",
       "      <td>Candace</td>\n",
       "      <td>it works pretty good for moving to different a...</td>\n",
       "      <td>1395964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Both sides are magnetic.  A real plus when you...</td>\n",
       "      <td>01 28, 2013</td>\n",
       "      <td>A21KH420DK0ICA</td>\n",
       "      <td>capemaychristy</td>\n",
       "      <td>love this!</td>\n",
       "      <td>1359331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought one a few years ago for my daughter and...</td>\n",
       "      <td>02 8, 2014</td>\n",
       "      <td>AR29QK6HPFYZ4</td>\n",
       "      <td>dcrm</td>\n",
       "      <td>Daughters love it</td>\n",
       "      <td>1391817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I have a stainless steel refrigerator therefor...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>ACCH8EOML6FN5</td>\n",
       "      <td>DoyZ</td>\n",
       "      <td>Great to have so he can play with his alphabet...</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  0439893577  [0, 0]      5.0   \n",
       "1  0439893577  [1, 1]      4.0   \n",
       "2  0439893577  [1, 1]      5.0   \n",
       "3  0439893577  [0, 0]      5.0   \n",
       "4  0439893577  [1, 1]      4.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I like the item pricing. My granddaughter want...  01 29, 2014   \n",
       "1  Love the magnet easel... great for moving to d...  03 28, 2014   \n",
       "2  Both sides are magnetic.  A real plus when you...  01 28, 2013   \n",
       "3  Bought one a few years ago for my daughter and...   02 8, 2014   \n",
       "4  I have a stainless steel refrigerator therefor...   05 5, 2014   \n",
       "\n",
       "       reviewerID    reviewerName  \\\n",
       "0  A1VXOAVRGKGEAK           Angie   \n",
       "1   A8R62G708TSCM         Candace   \n",
       "2  A21KH420DK0ICA  capemaychristy   \n",
       "3   AR29QK6HPFYZ4            dcrm   \n",
       "4   ACCH8EOML6FN5            DoyZ   \n",
       "\n",
       "                                             summary  unixReviewTime  \n",
       "0                                     Magnetic board      1390953600  \n",
       "1  it works pretty good for moving to different a...      1395964800  \n",
       "2                                         love this!      1359331200  \n",
       "3                                  Daughters love it      1391817600  \n",
       "4  Great to have so he can play with his alphabet...      1399248000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at product id `asin` and `overall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_jf = jf[['asin', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='0439893577', overall=5.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_jf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, overall: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_jf.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(jf, n=5):\n",
    "    '''load portion of json dataframe into a pandas dataframe'''\n",
    "    return jf.limit(n).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall\n",
       "0  0439893577      5.0\n",
       "1  0439893577      4.0\n",
       "2  0439893577      5.0\n",
       "3  0439893577      5.0\n",
       "4  0439893577      4.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(reviews_jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_review_jf = reviews_jf.sort('overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000ZLZ1NU</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0010SGZEG</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0010AYFPU</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000Z9FT0M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0010EJGSC</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall\n",
       "0  B000ZLZ1NU      1.0\n",
       "1  B0010SGZEG      1.0\n",
       "2  B0010AYFPU      1.0\n",
       "3  B000Z9FT0M      1.0\n",
       "4  B0010EJGSC      1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(sorted_review_jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sql language to return what we want. This has the advantage over pandas in that you can access data of unlimited size if it's within our cluster, while pandas needs to access it all locally. Let's find out the values_count of sorted_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT overall, COUNT (*)\n",
    "FROM reviews\n",
    "GROUP BY overall'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_jf.createOrReplaceTempView('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>37445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>102790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  count(1)\n",
       "0      1.0      4707\n",
       "1      4.0     37445\n",
       "2      3.0     16357\n",
       "3      2.0      6298\n",
       "4      5.0    102790"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[59] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[61] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jf.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Spark objects are immutable, while pandas are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_one = jf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='0439893577', helpful=[0, 0], overall=5.0, reviewText='I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.', reviewTime='01 29, 2014', reviewerID='A1VXOAVRGKGEAK', reviewerName='Angie', summary='Magnetic board', unixReviewTime=1390953600)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_one['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words in `row_one` `reviewText`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(row_one['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to count the words in all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_col = jf['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mjf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` by adding a column or replacing the\n",
       "existing column that has the same name.\n",
       "\n",
       "The column expression must be an expression over this DataFrame; attempting to add\n",
       "a column from some other dataframe will raise an error.\n",
       "\n",
       ":param colName: string, name of the new column.\n",
       ":param col: a :class:`Column` expression for the new column.\n",
       "\n",
       ">>> df.withColumn('age2', df.age + 2).collect()\n",
       "[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
       "\n",
       ".. versionadded:: 1.3\n",
       "\u001b[0;31mFile:\u001b[0m      /anaconda3/envs/learn-env/lib/python3.6/site-packages/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jf.withColumn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a user defined function (UDF).\n",
       "\n",
       ".. note:: The user-defined functions are considered deterministic by default. Due to\n",
       "    optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
       "    more times than it is present in the query. If your function is not deterministic, call\n",
       "    `asNondeterministic` on the user defined function. E.g.:\n",
       "\n",
       ">>> from pyspark.sql.types import IntegerType\n",
       ">>> import random\n",
       ">>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
       "\n",
       ".. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
       "    in boolean expressions and it ends up with being executed all internally. If the functions\n",
       "    can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
       "\n",
       ".. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
       "\n",
       ":param f: python function if used as a standalone function\n",
       ":param returnType: the return type of the user-defined function. The value can be either a\n",
       "    :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
       "\n",
       ">>> from pyspark.sql.types import IntegerType\n",
       ">>> slen = udf(lambda s: len(s), IntegerType())\n",
       ">>> @udf\n",
       "... def to_upper(s):\n",
       "...     if s is not None:\n",
       "...         return s.upper()\n",
       "...\n",
       ">>> @udf(returnType=IntegerType())\n",
       "... def add_one(x):\n",
       "...     if x is not None:\n",
       "...         return x + 1\n",
       "...\n",
       ">>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
       ">>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
       "+----------+--------------+------------+\n",
       "|slen(name)|to_upper(name)|add_one(age)|\n",
       "+----------+--------------+------------+\n",
       "|         8|      JOHN DOE|          22|\n",
       "+----------+--------------+------------+\n",
       "\n",
       ".. versionadded:: 1.3\n",
       "\u001b[0;31mFile:\u001b[0m      /anaconda3/envs/learn-env/lib/python3.6/site-packages/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.udf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "word_count_ujf = F.udf(word_count, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_jf = jf.withColumn('wordcount', word_count_udf(review_text_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='0439893577', helpful=[0, 0], overall=5.0, reviewText='I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.', reviewTime='01 29, 2014', reviewerID='A1VXOAVRGKGEAK', reviewerName='Angie', summary='Magnetic board', unixReviewTime=1390953600, wordcount=20)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_jf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use sql language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.word_count(text)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jf.createOrReplaceTempView('reviews')\n",
    "spark.udf.register('word_count', word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT asin\n",
    ", overall\n",
    ", reviewText\n",
    ", word_count(reviewText)\n",
    "FROM reviews'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_jf = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. create python function <br>\n",
    "def show(df, n=5): <br>\n",
    "    '''load portion of json dataframe into a pandas dataframe''' <br>\n",
    "    return df.limit(n).toPandas() <br>\n",
    "2. wrap it with something <br>\n",
    "import pyspark.sql.functions as F <br>\n",
    "from pyspark.sql.types import IntegerType <br>\n",
    "word_count_udf = F.udf(word_count, IntegerType()) <br>\n",
    "3. Create name for the dataframe <br>\n",
    "df.createOrReplaceTempView('reviews') <br>\n",
    "4. Register that function <br>\n",
    "spark.udf.register('word_count', word_count) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_the_things(text):\n",
    "    return [len(text.split), len(tect.split())]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "count_ujf = F.udf(count_all_the_things, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_jf = jf.withColumn('counts', count_ujf(jf['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint, counts: array<int>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_jf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn.co lab stuff:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SparkContext - Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArrayType',\n",
       " 'F',\n",
       " 'In',\n",
       " 'IntegerType',\n",
       " 'Out',\n",
       " '_',\n",
       " '_11',\n",
       " '_13',\n",
       " '_14',\n",
       " '_16',\n",
       " '_18',\n",
       " '_2',\n",
       " '_23',\n",
       " '_24',\n",
       " '_25',\n",
       " '_27',\n",
       " '_28',\n",
       " '_3',\n",
       " '_30',\n",
       " '_36',\n",
       " '_37',\n",
       " '_43',\n",
       " '_5',\n",
       " '_7',\n",
       " '_8',\n",
       " '_9',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i26',\n",
       " '_i27',\n",
       " '_i28',\n",
       " '_i29',\n",
       " '_i3',\n",
       " '_i30',\n",
       " '_i31',\n",
       " '_i32',\n",
       " '_i33',\n",
       " '_i34',\n",
       " '_i35',\n",
       " '_i36',\n",
       " '_i37',\n",
       " '_i38',\n",
       " '_i39',\n",
       " '_i4',\n",
       " '_i40',\n",
       " '_i41',\n",
       " '_i42',\n",
       " '_i43',\n",
       " '_i44',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'count_all_the_things',\n",
       " 'count_udf',\n",
       " 'counts_df',\n",
       " 'df',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'output',\n",
       " 'pf',\n",
       " 'pyspark',\n",
       " 'query',\n",
       " 'quit',\n",
       " 'review_text_col',\n",
       " 'reviews_df',\n",
       " 'row_one',\n",
       " 'sc',\n",
       " 'show',\n",
       " 'sorted_review_df',\n",
       " 'spark',\n",
       " 'word_count',\n",
       " 'word_count_udf']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(builtins.object)\n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create L{RDD} and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through C{conf}.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A L{SparkConf} object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an L{Accumulator} with the given initial value, using a given\n",
      " |      L{AccumulatorParam} helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The C{path} passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use\n",
      " |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The C{path} passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a\n",
      " |      L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      L{setLocalProperty}\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be a HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobDescription(self, value)\n",
      " |      Set a human readable description of the current job.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> suppress = threading.Thread(target=stop_job).start()\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      ['Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files::\n",
      " |      \n",
      " |        hdfs://a-hdfs-path/part-00000\n",
      " |        hdfs://a-hdfs-path/part-00001\n",
      " |        ...\n",
      " |        hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
      " |      then C{rdd} contains::\n",
      " |      \n",
      " |        (a-hdfs-path/part-00000, its content)\n",
      " |        (a-hdfs-path/part-00001, its content)\n",
      " |        ...\n",
      " |        (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [('.../1.txt', '1'), ('.../2.txt', '2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(conf=None) from builtins.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(key, value) from builtins.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n",
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      self != 0\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(help (sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to check a few spark context attributes including SparkContext.version and SparkContext.defaultParalellism to check the current version of Apache Spark and number of cores being used for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark-shell'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access complete configuration settings (including all defaults) for the current spark context using _conf.getAll() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark Context can be shut down using SparkContext.stop() method. Let's use this method to shut down the current spark context. In this case `sc.stop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs) - Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(range(1000))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#create rdd with 10 partitions\n",
    "rdd = sc.parallelize(data, numSlices=10)\n",
    "print(type(rdd))\n",
    "# <class 'pyspark.rdd.RDD'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic descriptive RDD actions\n",
    "Let's perform some basic operations on our RDD. In the cell below, use the methods: <br>\n",
    "\n",
    "count: returns the total count of items in the RDD <br>\n",
    "first: returns the first item in the RDD <br>\n",
    "take: returns the first n items in the RDD <br>\n",
    "top: returns the top n items <br>\n",
    "collect: returns everything from your RDD<br>\n",
    "\n",
    "It's important to note that in a big data context, calling the collect method will often take a very long time to execute and should be handled with care!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[999, 998, 997, 996, 995, 994]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.array(range(1,1001))\n",
    "sales_figures = nums * np.random.rand(1000)\n",
    "# sales_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an RDD called price_items using the newly created data with 10 slices.\n",
    "price_items = sc.parallelize(sales_figures, numSlices=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36798328687304893,\n",
       " 1.4905084514422406,\n",
       " 2.1266286810769466,\n",
       " 2.6200923771108107,\n",
       " 1.2412576789528034,\n",
       " 0.6763437909253847]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_items.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[968.9029422152379,\n",
       " 965.951440353942,\n",
       " 909.1750931567626,\n",
       " 895.2822647884403,\n",
       " 887.8316179652227,\n",
       " 877.2716755368646,\n",
       " 864.6452287155496,\n",
       " 854.7135591488441,\n",
       " 852.8696867798243,\n",
       " 846.2444427560786,\n",
       " 844.2846080127582,\n",
       " 842.5498471657254,\n",
       " 841.0961082873267,\n",
       " 831.1327685836549,\n",
       " 829.8606722039958]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_items.top(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform some operations on this simple dataset. To begin with, create a function that will take into account how much money BuyStuff will receive after sales tax has been applied (assume a sales tax of 8%). To make this happen, create a function called sales_tax that returns the amount of money our company will receive after the sale tax has been applied. The function will have this parameter:\n",
    "\n",
    "item: (float) number to be multiplied by the sales tax. <br>\n",
    "\n",
    "Apply that function to the rdd by using the map method and assign it to a variable renenue_minus_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_tax(num):\n",
    "    return num*.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_minus_tax = price_items.map(sales_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228238.12791059093"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_minus_tax.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[891.3907068380189,\n",
       " 888.6753251256266,\n",
       " 836.4410857042217,\n",
       " 823.6596836053651,\n",
       " 816.805088528005]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue_minus_tax.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also use lambda functions if you want to quickly perform simple operations on data without creating a function. Let's assume that BuyStuff has also decided to offer a 10% discount on all of their items on the pre-tax amounts of each item. Use a lambda function within a map method to apply the additional 10% loss in revenue for BuyStuff and assign the transformed RDD to a new RDD called discounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted = revenue_minus_tax.map(lambda x : x*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205414.31511953185"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also able to chain methods together with Spark. In one line, remove the tax and discount from the revenue of BuyStuff then use a collection method to see the 15 costliest items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[802.2516361542171,\n",
       " 799.807792613064,\n",
       " 752.7969771337995,\n",
       " 741.2937152448286,\n",
       " 735.1245796752045,\n",
       " 726.380947344524,\n",
       " 715.9262493764751,\n",
       " 707.7028269752429,\n",
       " 706.1761006536946,\n",
       " 700.690398602033,\n",
       " 699.0676554345638,\n",
       " 697.6312734532207,\n",
       " 696.4275776619066,\n",
       " 688.1779323872663,\n",
       " 687.1246365849087]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_items.map(sales_tax).map(lambda x : x*0.9).top(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[802.2516361542171,\n",
       " 799.807792613064,\n",
       " 752.7969771337995,\n",
       " 741.2937152448286,\n",
       " 735.1245796752045,\n",
       " 726.380947344524,\n",
       " 715.9262493764751,\n",
       " 707.7028269752429,\n",
       " 706.1761006536946,\n",
       " 700.690398602033,\n",
       " 699.0676554345638,\n",
       " 697.6312734532207,\n",
       " 696.4275776619066,\n",
       " 688.1779323872663,\n",
       " 687.1246365849087]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted.top(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to see the full lineage of all the operations that have been performed on an RDD by using the RDD.toDebugString() method. As your transformations become more complex, you are encouraged to call this method to get a better understanding of the dependencies between RDDs. Try calling it on the discounted RDD to see what RDDs it is dependent on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(10) PythonRDD[112] at RDD at PythonRDD.scala:53 []\\n |   ParallelCollectionRDD[98] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map vs. Flatmap <br>\n",
    "Depending on how you want your data to be outputted, you might want to use flatMap rather than a simple map. Let's take a look at how it performs operations versus the standard map. Let's say we wanted to maintain the original amount BuyStuff receives for each item as well as the new amount after the tax and discount are applied. Create a map function that will a tuple with (original price, post-discount price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[(968.9029422152379, 802.2516361542171), (965.951440353942, 799.807792613064), (909.1750931567626, 752.7969771337995), (895.2822647884403, 741.2937152448286), (887.8316179652227, 735.1245796752045), (877.2716755368646, 726.380947344524), (864.6452287155496, 715.9262493764751), (854.7135591488441, 707.7028269752429), (852.8696867798243, 706.1761006536946), (846.2444427560786, 700.690398602033)]\n"
     ]
    }
   ],
   "source": [
    "mapped = price_items.map(lambda x: (x, x*0.92 *0.9))\n",
    "print(mapped.count())\n",
    "print(mapped.top(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have 1000 tuples created to our specification. Let's take a look at how flatMap differs in its implementation. Use the flatMap method with the same function you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[968.9029422152379, 965.951440353942, 909.1750931567626, 895.2822647884403, 887.8316179652227, 877.2716755368646, 864.6452287155496, 854.7135591488441, 852.8696867798243, 846.2444427560786]\n"
     ]
    }
   ],
   "source": [
    "flat_mapped = price_items.flatMap(lambda x : (x, x*0.92*0.9 ))\n",
    "print(flat_mapped.count())\n",
    "print(flat_mapped.top(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter <br>\n",
    "After meeting with some external consultants, BuyStuff has determined that its business will be more profitable if it focuses on higher ticket items. Now, use the filter method to select items that bring in more than $300 after tax and discount have been removed. A filter method is a specialized form of a map function that only returns the items that match a certain criteria. In the cell below: <br>\n",
    "\n",
    "use a lambda function within a filter function to meet the consultant's suggestion's specifications. set RDD = selected_items <br>\n",
    "calculate the total number of items remaining in BuyStuff's inventory <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_items = discounted.filter(lambda x: x>300)\n",
    "selected_items.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce <br>\n",
    "Now it's time to figure out how much money BuyStuff would make from selling one of all of it's items after they've reduced their inventory. Use a reduce method with a lambda function to to add up all of the values in the RDD. Your lambda function should have two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124934.06646485813"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_items.reduce(lambda x,y :x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124934.06646485813"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "selected_items.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has come for BuyStuff to open up shop and start selling it's goods. It only has one of each item, but it's allowing 50 lucky users to buy as many items as they want while they remain in stock. Within seconds, BuyStuff is sold out. Below, you'll find the sales data in an RDD with tuples of (user, item bought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 303.4624427773786),\n",
       " (10, 315.6169912192194),\n",
       " (13, 337.7230685094742),\n",
       " (11, 348.07129244539885),\n",
       " (37, 342.4714746018663),\n",
       " (15, 350.23475907942833),\n",
       " (15, 334.146155059639)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "# generating simulated users that have bought each item\n",
    "sales_data = selected_items.map(lambda x: (random.randint(1,50),x))\n",
    "\n",
    "sales_data.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to determine some basic statistics about BuyStuff users.\n",
    "\n",
    "Let's start off by creating an RDD that determines how much each user spent in total. To do this we can use a method called reduceByKey to perform reducing operations while grouping by keys. After you have calculated the total, use the sortBy method on the RDD to rank the users from highest spending to least spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 11542.767611185882),\n",
       " (10, 3707.4074331473635),\n",
       " (40, 1885.1598469952055),\n",
       " (31, 5252.41224347783),\n",
       " (11, 4267.497122488841),\n",
       " (41, 4711.003807706706),\n",
       " (1, 1093.0944176573632),\n",
       " (2, 6889.602162272571),\n",
       " (32, 2363.8882710575795),\n",
       " (12, 6574.032284886426)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_spent = sales_data.reduceByKey(lambda x,y :x + y)\n",
    "total_spent.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 11542.767611185882),\n",
       " (23, 10436.863310364082),\n",
       " (2, 6889.602162272571),\n",
       " (48, 6678.452110419206),\n",
       " (12, 6574.032284886426),\n",
       " (15, 5805.965089662124),\n",
       " (6, 5618.594660703813),\n",
       " (31, 5252.41224347783),\n",
       " (47, 4955.19240321956),\n",
       " (37, 4806.55955888658),\n",
       " (41, 4711.003807706706),\n",
       " (46, 4633.458945130777),\n",
       " (4, 4518.740547895117),\n",
       " (11, 4267.497122488841),\n",
       " (17, 3800.2752688267865),\n",
       " (10, 3707.4074331473635),\n",
       " (45, 3399.9164830691407),\n",
       " (7, 2601.3877711336913),\n",
       " (29, 2529.2351428588377),\n",
       " (35, 2491.7212012131413),\n",
       " (32, 2363.8882710575795),\n",
       " (44, 2257.807667032602),\n",
       " (28, 2144.744044546806),\n",
       " (24, 2141.355373192837),\n",
       " (40, 1885.1598469952055),\n",
       " (18, 1732.0120341038255),\n",
       " (22, 1636.7996931104503),\n",
       " (5, 1451.0508290516796),\n",
       " (13, 1117.4646435338743),\n",
       " (1, 1093.0944176573632),\n",
       " (9, 799.807792613064),\n",
       " (36, 651.8393595868904),\n",
       " (16, 437.95733382747915)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_spent.sortBy(lambda x: x[1],ascending = False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's determine how many items were bought per user. This can be solved in one line using an RDD method. After you've counted the total number of items bought per person, sort the users from most number of items bought to least number of items. Time to start a customer loyalty program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 25),\n",
       " (23, 24),\n",
       " (2, 16),\n",
       " (48, 15),\n",
       " (12, 15),\n",
       " (6, 12),\n",
       " (31, 12),\n",
       " (46, 11),\n",
       " (47, 11),\n",
       " (15, 11),\n",
       " (37, 10),\n",
       " (4, 10),\n",
       " (11, 9),\n",
       " (41, 9),\n",
       " (45, 8),\n",
       " (10, 8),\n",
       " (17, 8),\n",
       " (32, 6),\n",
       " (29, 6),\n",
       " (7, 6),\n",
       " (44, 6),\n",
       " (35, 5),\n",
       " (24, 5),\n",
       " (40, 4),\n",
       " (28, 4),\n",
       " (18, 4),\n",
       " (22, 4),\n",
       " (13, 3),\n",
       " (1, 2),\n",
       " (5, 2),\n",
       " (16, 1),\n",
       " (9, 1),\n",
       " (36, 1)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_items = sales_data.countByKey()\n",
    "sorted(total_items.items(),key=lambda kv:kv[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(6, 12), (2, 16), (37, 10), (23, 24), (32, 6), (46, 11), (29, 6), (7, 6), (50, 25), (48, 15), (31, 12), (44, 6), (12, 15), (47, 11), (45, 8), (4, 10), (11, 9), (10, 8), (35, 5), (24, 5), (15, 11), (41, 9), (40, 4), (17, 8), (28, 4), (18, 4), (22, 4), (13, 3), (1, 2), (5, 2), (16, 1), (9, 1), (36, 1)])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_items.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count with Map-Reduce - Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
